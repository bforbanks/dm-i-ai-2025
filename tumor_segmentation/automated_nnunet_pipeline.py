#!/usr/bin/env python3
"""
Automated nnUNet Pipeline Script

This script automates the complete nnUNet pipeline:
1. Create fresh ResEnc model with custom name
2. Fix ResEnc configuration to remove resampling with custom tiling
3. Run nnUNetv2_preprocess with the custom configuration
4. Create stratified split with custom percentage

All parameters are configurable at the top of the script.

MODEL SIZE OPTIONS:
- STANDARD: Uses default nnUNet architecture (7 stages, 32-512 features)
- SMALLER: Uses reduced architecture (5 stages, 16-256 features) - ~75% fewer parameters
- VERY SMALL: Uses minimal architecture (4 stages, 16-128 features) - ~90% fewer parameters

To use different sizes, modify the SMALLER_MODEL_CONFIG section below.
"""

import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Tuple, Optional
import random
import numpy as np
import cv2
from batchgenerators.utilities.file_and_folder_operations import load_json, save_json

# =============================================================================
# CONFIGURABLE PARAMETERS - MODIFY THESE AS NEEDED
# =============================================================================

# Model configuration
MODEL_NAME = "test-CV"  # Name for the new ResEnc model
TILING_SIZE = (
    320,
    320,
)  # Tiling size (width, height) in pixels - reduced to avoid dimension mismatches
STRATIFIED_SPLIT_PERCENTAGE = (
    0.3  # Percentage for stratified split (0.0-1.0) - REDUCED to prevent overfitting
)
# Dataset configuration
DATASET_ID = 1  # Dataset ID (e.g., 1 for Dataset001_TumorSegmentation)
BATCH_SIZE = 8  # Batch size for training
RANDOM_SEED = 42  # Random seed for reproducibility

# Quick testing mode - SET TO True FOR RAPID TESTING
QUICK_TEST_MODE = True  # Reduces max epochs and makes early stopping very aggressive

# Select which configuration to use
# Options: "standard", "reduced", "minimal", "conservative", "tiny", or None for standard
SELECTED_MODEL_SIZE = "reduced"  # CHANGED from conservative to reduced to prevent overfitting


# =============================================================================
# MODEL SIZE CONFIGURATION - REDUCE THESE FOR SMALLER MODELS
# =============================================================================

# Enhanced model configurations with parameter estimates
ENHANCED_SMALLER_CONFIGS = {
    "standard": None,  # Use default nnUNet architecture
    "conservative": {
        "n_stages": 6,
        "features_per_stage": [16, 32, 64, 128, 256, 320],
        "n_blocks_per_stage": [1, 2, 2, 3, 3, 3],
        "n_conv_per_stage_decoder": [1, 1, 1, 1, 1],
        "description": "Conservatively reduced model (~50% fewer parameters)"
    },
    "reduced": {
        "n_stages": 5,
        "features_per_stage": [16, 32, 64, 128, 256],
        "n_blocks_per_stage": [1, 2, 3, 4, 4],
        "n_conv_per_stage_decoder": [1, 1, 1, 1],
        "description": "Moderately reduced model (~75% fewer parameters)"
    },
    "minimal": {
        "n_stages": 4,
        "features_per_stage": [16, 32, 64, 128],
        "n_blocks_per_stage": [1, 2, 2, 3],
        "n_conv_per_stage_decoder": [1, 1, 1],
        "description": "Heavily reduced model (~90% fewer parameters)"
    },
    "tiny": {
        "n_stages": 3,
        "features_per_stage": [16, 32, 64],
        "n_blocks_per_stage": [1, 1, 2],
        "n_conv_per_stage_decoder": [1, 1],
        "description": "Extremely small model (~95% fewer parameters)"
    }
}

# Set the active configuration
if SELECTED_MODEL_SIZE in ENHANCED_SMALLER_CONFIGS:
    SMALLER_MODEL_CONFIG = ENHANCED_SMALLER_CONFIGS[SELECTED_MODEL_SIZE]
else:
    SMALLER_MODEL_CONFIG = None


def calculate_model_parameters(config):
    """
    Estimate parameter count for the model configuration.
    
    This provides a simplified estimation of the total parameters in the ResidualEncoderUNet
    based on the configuration parameters. Useful for comparing model sizes.
    
    Args:
        config: Dictionary containing model configuration with keys:
                - features_per_stage: List of feature counts per stage
                - n_blocks_per_stage: List of residual blocks per stage
                - n_conv_per_stage_decoder: List of decoder convolutions per stage
    
    Returns:
        int: Estimated total number of parameters
    """
    if config is None:
        # Rough estimate for standard nnUNet (7 stages, features: 32->512)
        return 35_000_000  # ~35M parameters for standard model
    
    total_params = 0
    input_channels = 1  # Assuming single modality input (modify if multi-modal)
    
    # Encoder parameters estimation
    for i, (features, blocks) in enumerate(zip(config['features_per_stage'], 
                                               config['n_blocks_per_stage'])):
        # Initial convolution for this stage
        if i == 0:
            # First stage: input channels to first feature count
            conv_params = features * input_channels * 3 * 3 * 3
        else:
            # Subsequent stages: previous features to current features
            prev_features = config['features_per_stage'][i-1]
            conv_params = features * prev_features * 3 * 3 * 3
        
        total_params += conv_params
        
        # Residual blocks parameters
        # Each residual block has 2 convolutions: features -> features
        residual_params = blocks * features * features * 3 * 3 * 3 * 2
        total_params += residual_params
        
        # Instance normalization parameters (scale + bias)
        norm_params = features * 2 * (1 + blocks)  # Initial conv + residual blocks
        total_params += norm_params
    
    # Decoder parameters estimation
    n_decoder_stages = len(config['n_conv_per_stage_decoder'])
    for i, decoder_convs in enumerate(config['n_conv_per_stage_decoder']):
        stage_idx = len(config['features_per_stage']) - 2 - i  # Decoder works backwards
        if stage_idx >= 0:
            current_features = config['features_per_stage'][stage_idx]
            if stage_idx < len(config['features_per_stage']) - 1:
                next_features = config['features_per_stage'][stage_idx + 1]
            else:
                next_features = current_features
            
            # Upsampling convolution
            upsample_params = current_features * next_features * 3 * 3 * 3
            total_params += upsample_params
            
            # Decoder convolutions
            decoder_params = decoder_convs * current_features * current_features * 3 * 3 * 3
            total_params += decoder_params
            
            # Normalization parameters
            norm_params = current_features * 2 * decoder_convs
            total_params += norm_params
    
    # Final output layer (assuming binary segmentation, modify for multi-class)
    if len(config['features_per_stage']) > 0:
        output_params = config['features_per_stage'][0] * 2 * 1 * 1 * 1  # 2 classes (background + tumor)
        total_params += output_params
    
    return int(total_params)

# Print model configuration info
def print_model_info():
    """Print information about the selected model configuration."""
    if SMALLER_MODEL_CONFIG is None:
        estimated_params = calculate_model_parameters(None)
        print(f"üîß Using STANDARD nnUNet architecture")
        print(f"   Estimated parameters: ~{estimated_params/1_000_000:.1f}M")
    else:
        estimated_params = calculate_model_parameters(SMALLER_MODEL_CONFIG)
        config_name = SELECTED_MODEL_SIZE.upper()
        description = SMALLER_MODEL_CONFIG.get('description', 'Custom configuration')
        
        print(f"üîß Using {config_name} model configuration")
        print(f"   Description: {description}")
        print(f"   Stages: {SMALLER_MODEL_CONFIG['n_stages']}")
        print(f"   Features per stage: {SMALLER_MODEL_CONFIG['features_per_stage']}")
        print(f"   Blocks per stage: {SMALLER_MODEL_CONFIG['n_blocks_per_stage']}")
        print(f"   Estimated parameters: ~{estimated_params/1_000_000:.1f}M")
        
        # Compare to standard
        standard_params = calculate_model_parameters(None)
        reduction = (1 - estimated_params / standard_params) * 100
        print(f"   Parameter reduction: {reduction:.1f}% vs standard model")


# =============================================================================
# OVERFITTING PREVENTION STRATEGIES - Based on nnUNet Research
# =============================================================================

def add_overfitting_prevention_config(plans, config_name):
    """
    Add overfitting prevention strategies based on nnUNet research.
    
    Based on findings from:
    - nnUNet BraTS paper: More aggressive data augmentation helps
    - nnUNet original paper: Proper validation and early stopping crucial
    
    Args:
        plans: The nnUNet plans dictionary
        config_name: Name of the configuration to modify
    """
    
    if config_name not in plans["configurations"]:
        print(f"‚ö†Ô∏è  Configuration '{config_name}' not found for overfitting prevention")
        return plans
    
    config = plans["configurations"][config_name]
    
    # Add overfitting prevention settings
    # Adjust settings based on quick test mode
    if QUICK_TEST_MODE:
        max_epochs = 50      # Very short for quick testing
        patience = 5         # Stop very quickly if no improvement
        lr_patience = 3      # Reduce LR quickly
        print("üöÄ QUICK TEST MODE: Dramatically reduced training time")
    else:
        max_epochs = 1000    # Standard nnUNet
        patience = 10        # Conservative early stopping
        lr_patience = 5      # Moderate LR reduction
    
    overfitting_prevention = {
        # Early stopping strategy (adaptive based on mode)
        "num_epochs": max_epochs,
        "patience": patience,
        "lr_patience": lr_patience,
        
        # Data augmentation enhancement (based on BraTS paper)
        "data_augmentation": {
            "rotation": {"probability": 0.3, "range": (-15, 15)},  # Increased from 0.2
            "scaling": {"probability": 0.3, "range": (0.65, 1.6)},  # More aggressive
            "elastic_deformation": {"probability": 0.3},           # Added based on BraTS
            "brightness": {"probability": 0.3},                    # Added
            "gamma": {"probability": 0.5, "range": (0.7, 1.5)},   # More aggressive
            "mirror": {"probability": 0.5}                         # Standard
        },
        
        # Regularization (nnUNet standard + enhancements)
        "weight_decay": 1e-5,        # L2 regularization
        "instance_normalization": True,  # Proven better than batch norm for small batches
        "dropout_replacement": "none",    # nnUNet doesn't use dropout, uses other strategies
        
        # Validation strategy
        "validation_frequency": 5,    # Validate every 5 epochs
        "cross_validation": True,     # Use 5-fold CV as per nnUNet standard
        
        # Memory and computational efficiency
        "mixed_precision": True,      # Reduces memory usage
        "deterministic": True,        # For reproducibility
    }
    
    # Add to configuration
    config["overfitting_prevention"] = overfitting_prevention
    
    print(f"‚úÖ Added overfitting prevention strategies to '{config_name}'")
    print(f"   - Enhanced data augmentation (based on BraTS paper)")
    print(f"   - Max epochs: {overfitting_prevention['num_epochs']}")
    print(f"   - Early stopping (patience: {overfitting_prevention['patience']} epochs)")
    print(f"   - LR reduction (patience: {overfitting_prevention['lr_patience']} epochs)")
    print(f"   - Proper validation strategy (every {overfitting_prevention['validation_frequency']} epochs)")
    print(f"   - L2 regularization (weight_decay: {overfitting_prevention['weight_decay']})")
    
    return plans


def create_cross_validation_splits(dataset_id: int, seed: int, n_folds: int = 5, tumor_only_validation: bool = False):
    """
    Create proper 5-fold cross-validation splits as per nnUNet standard.
    
    IMPORTANT: If test set contains only tumor cases, set tumor_only_validation=True
    to better match test distribution and prevent overfitting to controls.
    
    Args:
        dataset_id: Dataset ID
        seed: Random seed for reproducibility
        n_folds: Number of folds (nnUNet standard is 5)
        tumor_only_validation: If True, validation sets contain only tumor cases
    """
    random.seed(seed)
    np.random.seed(seed)
    
    print(f"üîÑ Creating {n_folds}-fold cross-validation splits (nnUNet standard)")
    print(f"   This helps prevent overfitting through proper validation")
    
    # Set up paths
    nnUNet_preprocessed = os.environ.get("nnUNet_preprocessed")
    if not nnUNet_preprocessed:
        script_dir = Path(__file__).parent
        project_dir = script_dir.parent
        nnUNet_preprocessed = str(
            project_dir / "tumor_segmentation" / "data_nnUNet" / "preprocessed"
        )
        os.environ["nnUNet_preprocessed"] = nnUNet_preprocessed
    
    # Get dataset path for raw data
    nnUNet_raw = os.environ.get("nnUNet_raw")
    if not nnUNet_raw:
        script_dir = Path(__file__).parent
        project_dir = script_dir.parent
        nnUNet_raw = str(project_dir / "tumor_segmentation" / "data_nnUNet")
        os.environ["nnUNet_raw"] = nnUNet_raw
    
    dataset_name = f"Dataset{dataset_id:03d}_TumorSegmentation"
    dataset_path = Path(str(nnUNet_raw)) / dataset_name
    preprocessed_path = Path(str(nnUNet_preprocessed)) / dataset_name
    
    # Get all training cases
    images_tr_dir = dataset_path / "imagesTr"
    if not images_tr_dir.exists():
        raise FileNotFoundError(f"ImagesTr directory not found: {images_tr_dir}")
    
    # Get all image files and extract case names
    image_files = list(images_tr_dir.glob("*_0000.png"))
    all_cases = []
    
    for img_file in image_files:
        case_name = img_file.stem.replace("_0000", "")
        all_cases.append(case_name)
    
    # Sort for reproducibility
    all_cases.sort()
    
    if tumor_only_validation:
        # Separate tumor and control cases based on naming convention
        # Assuming tumor cases have specific naming (adjust as needed)
        labels_tr_dir = dataset_path / "labelsTr"
        tumor_cases = []
        control_cases = []
        
        for case in all_cases:
            label_file = labels_tr_dir / f"{case}.png"
            if label_file.exists():
                # Read label to check if it has tumor (non-zero pixels)
                import cv2
                label_img = cv2.imread(str(label_file), cv2.IMREAD_GRAYSCALE)
                if label_img is not None and np.any(label_img.astype(int) > 0):
                    tumor_cases.append(case)
                else:
                    control_cases.append(case)
            else:
                # If no label file, assume control
                control_cases.append(case)
        
        print(f"üéØ TUMOR-ONLY VALIDATION MODE (matches test distribution)")
        print(f"üìä Cross-validation setup:")
        print(f"   Total cases: {len(all_cases)} ({len(tumor_cases)} tumor, {len(control_cases)} control)")
        print(f"   Folds: {n_folds}")
        print(f"   Validation: Only tumor cases (~{len(tumor_cases) // n_folds} per fold)")
        print(f"   Training: Tumor + Control cases")
        
        # Create tumor-only validation folds
        random.shuffle(tumor_cases)  # Shuffle tumor cases with fixed seed
        fold_size = len(tumor_cases) // n_folds
        folds = []
        
        for fold in range(n_folds):
            start_idx = fold * fold_size
            if fold == n_folds - 1:  # Last fold gets remaining cases
                end_idx = len(tumor_cases)
            else:
                end_idx = (fold + 1) * fold_size
            
            val_cases = tumor_cases[start_idx:end_idx]
            # Training includes ALL remaining tumor cases + ALL control cases
            train_tumor_cases = [case for case in tumor_cases if case not in val_cases]
            train_cases = train_tumor_cases + control_cases
            
            fold_split = {"train": train_cases, "val": val_cases}
            folds.append(fold_split)
            
            print(f"   Fold {fold}: {len(train_cases)} train ({len(train_tumor_cases)} tumor + {len(control_cases)} control), {len(val_cases)} val (tumor only)")
    
    else:
        # Standard cross-validation (original behavior)
        random.shuffle(all_cases)  # Shuffle with fixed seed
        
        print(f"üìä Cross-validation setup:")
        print(f"   Total cases: {len(all_cases)}")
        print(f"   Folds: {n_folds}")
        print(f"   Cases per fold: ~{len(all_cases) // n_folds}")
        
        # Create folds
        fold_size = len(all_cases) // n_folds
        folds = []
        
        for fold in range(n_folds):
            start_idx = fold * fold_size
            if fold == n_folds - 1:  # Last fold gets remaining cases
                end_idx = len(all_cases)
            else:
                end_idx = (fold + 1) * fold_size
            
            val_cases = all_cases[start_idx:end_idx]
            train_cases = [case for case in all_cases if case not in val_cases]
            
            fold_split = {"train": train_cases, "val": val_cases}
            folds.append(fold_split)
            
            print(f"   Fold {fold}: {len(train_cases)} train, {len(val_cases)} val")
    
    # Save the cross-validation splits
    cv_splits_file = preprocessed_path / "splits_final.json"
    save_json(folds, str(cv_splits_file))
    
    print(f"\n‚úÖ Cross-validation splits created: {cv_splits_file}")
    print(f"   Strategy: {n_folds}-fold CV (nnUNet standard for overfitting prevention)")
    
    return cv_splits_file


# Use smaller model to avoid dimension issues

# =============================================================================
# END OF CONFIGURABLE PARAMETERS
# =============================================================================


def create_fresh_resenc_plans(
    dataset_id: int,
    patch_size: Tuple[int, int],
    batch_size: int,
    config_name: str,
    smaller_model_config: Optional[dict] = None,
):
    """
    Create fresh ResEnc plans with optimized configuration.

    Args:
        dataset_id: Dataset ID (e.g., 1 for Dataset001_TumorSegmentation)
        patch_size: New patch size (height, width)
        batch_size: New batch size
        config_name: Name for the new configuration
        smaller_model_config: Optional dict with smaller model parameters
    """

    # Paths
    plans_dir = f"data_nnUNet/preprocessed/Dataset{dataset_id:03d}_TumorSegmentation"
    resenc_plans_path = f"{plans_dir}/nnUNetResEncUNetMPlans.json"

    # Check if ResEnc plans exist
    if not Path(resenc_plans_path).exists():
        print(f"‚ùå ResEnc plans not found: {resenc_plans_path}")
        print(
            "   Please run: nnUNetv2_plan_and_preprocess -d 1 -pl nnUNetPlannerResEncM"
        )
        return False

    print(f"üîÑ Creating fresh ResEnc configuration...")
    print(f"   ResEnc plans: {resenc_plans_path}")
    print(f"   New configuration: {config_name}")
    print(f"   Patch size: {patch_size}")
    print(f"   Batch size: {batch_size}")
    if smaller_model_config:
        print(f"   Architecture: ResidualEncoderUNet (SMALLER MODEL)")
        print(f"   Stages: {smaller_model_config['n_stages']}")
        print(f"   Features: {smaller_model_config['features_per_stage']}")
    else:
        print(f"   Architecture: ResidualEncoderUNet (improved)")

    # Load ResEnc plans
    with open(resenc_plans_path, "r") as f:
        plans = json.load(f)

    # Check if configuration already exists
    if config_name in plans["configurations"]:
        print(f"‚ö†Ô∏è  Configuration '{config_name}' already exists. Overwriting...")

    # Check if original 2d configuration exists
    if "2d" not in plans["configurations"]:
        print("‚ùå Original '2d' configuration not found in ResEnc plans")
        return False

    original_config = plans["configurations"]["2d"]

    # Calculate compatible patch size based on the ResEnc architecture
    # ResEnc has 7 stages with stride 2, so we need patch size divisible by 2^6 = 64
    def make_patch_size_compatible(patch_size, n_stages=7):
        """Make patch size compatible with the number of downsampling stages."""
        divisor = 2 ** (n_stages - 1)  # 2^6 = 64 for 7 stages
        compatible_size = []
        for dim in patch_size:
            # Round to nearest multiple of divisor
            rounded = round(dim / divisor) * divisor
            # Ensure minimum size
            if rounded < divisor:
                rounded = divisor
            compatible_size.append(rounded)
        return compatible_size

    # Determine number of stages for patch size calculation
    n_stages = 7  # Default ResEnc
    if smaller_model_config:
        n_stages = smaller_model_config["n_stages"]

    compatible_patch_size = make_patch_size_compatible(patch_size, n_stages)
    print(f"   Original patch size: {patch_size}")
    print(f"   Compatible patch size: {compatible_patch_size}")

    # Create fresh optimized configuration that inherits from ResEnc 2d
    optimized_config = {
        "inherits_from": "2d",
        "data_identifier": f"nnUNetPlans_{config_name}",
        "preprocessor_name": "DefaultPreprocessor",
        "batch_size": batch_size,
        "patch_size": compatible_patch_size,
        # Keep all other settings from original ResEnc config
        "median_image_size_in_voxels": original_config["median_image_size_in_voxels"],
        "spacing": original_config["spacing"],
        "normalization_schemes": original_config["normalization_schemes"],
        "use_mask_for_norm": original_config["use_mask_for_norm"],
        "batch_dice": original_config["batch_dice"],
    }

    # Create architecture configuration
    if smaller_model_config:
        # Use smaller model architecture
        architecture_config = {
            "network_class_name": "dynamic_network_architectures.architectures.unet.ResidualEncoderUNet",
            "arch_kwargs": {
                "n_stages": smaller_model_config["n_stages"],
                "features_per_stage": smaller_model_config["features_per_stage"],
                "conv_op": "torch.nn.modules.conv.Conv2d",
                "kernel_sizes": [[3, 3]] * smaller_model_config["n_stages"],
                "strides": [[1, 1]] + [[2, 2]] * (smaller_model_config["n_stages"] - 1),
                "n_blocks_per_stage": smaller_model_config["n_blocks_per_stage"],
                "n_conv_per_stage_decoder": smaller_model_config[
                    "n_conv_per_stage_decoder"
                ],
                "conv_bias": True,
                "norm_op": "torch.nn.modules.instancenorm.InstanceNorm2d",
                "norm_op_kwargs": {"eps": 1e-5, "affine": True},
                "dropout_op": None,
                "dropout_op_kwargs": None,
                "nonlin": "torch.nn.LeakyReLU",
                "nonlin_kwargs": {"inplace": True},
            },
            "_kw_requires_import": ["conv_op", "norm_op", "dropout_op", "nonlin"],
        }
        optimized_config["architecture"] = architecture_config
    else:
        # Keep the improved ResidualEncoderUNet architecture
        optimized_config["architecture"] = original_config["architecture"]

    # Add the new configuration to ResEnc plans
    plans["configurations"][config_name] = optimized_config

    # Apply overfitting prevention strategies
    print(f"üõ°Ô∏è  Applying overfitting prevention strategies...")
    plans = add_overfitting_prevention_config(plans, config_name)

    # Create backup before modifying
    backup_path = f"{plans_dir}/nnUNetResEncUNetMPlans_fresh_backup.json"
    with open(backup_path, "w") as f:
        json.dump(plans, f, indent=2)
    print(f"‚úÖ Backup created: {backup_path}")

    # Save modified ResEnc plans
    with open(resenc_plans_path, "w") as f:
        json.dump(plans, f, indent=2)

    print(f"‚úÖ Fresh ResEnc configuration created successfully!")
    print(f"   New configuration '{config_name}' added to {resenc_plans_path}")
    if smaller_model_config:
        print(f"   Using SMALLER ResidualEncoderUNet architecture")
        print(
            f"   Parameters: {smaller_model_config['n_stages']} stages, {sum(smaller_model_config['features_per_stage'])} total features"
        )
    else:
        print(f"   Using improved ResidualEncoderUNet architecture")

    return True


def fix_resenc_no_resampling(
    dataset_id: int, patch_size: Tuple[int, int], batch_size: int, config_name: str
):
    """
    Fix ResEnc configuration to remove resampling.

    Args:
        dataset_id: Dataset ID (e.g., 1 for Dataset001_TumorSegmentation)
        patch_size: New patch size (height, width)
        batch_size: New batch size
        config_name: Name for the configuration to fix
    """

    # Paths
    plans_dir = f"data_nnUNet/preprocessed/Dataset{dataset_id:03d}_TumorSegmentation"
    resenc_plans_path = f"{plans_dir}/nnUNetResEncUNetMPlans.json"

    # Check if ResEnc plans exist
    if not Path(resenc_plans_path).exists():
        print(f"‚ùå ResEnc plans not found: {resenc_plans_path}")
        return False

    print(f"üîß Fixing ResEnc configuration to remove resampling...")
    print(f"   ResEnc plans: {resenc_plans_path}")
    print(f"   Configuration: {config_name}")
    print(f"   Patch size: {patch_size}")
    print(f"   Batch size: {batch_size}")
    print(f"   Goal: Use original image dimensions (no resampling)")

    # Load ResEnc plans
    with open(resenc_plans_path, "r") as f:
        plans = json.load(f)

    # Check if configuration exists
    if config_name not in plans["configurations"]:
        print(f"‚ùå Configuration '{config_name}' not found in ResEnc plans")
        return False

    # Get the current configuration
    current_config = plans["configurations"][config_name]

    # Calculate compatible patch size (same function as in create_fresh_resenc_plans)
    def make_patch_size_compatible(patch_size, n_stages=7):
        """Make patch size compatible with the number of downsampling stages."""
        divisor = 2 ** (n_stages - 1)  # 2^6 = 64 for 7 stages
        compatible_size = []
        for dim in patch_size:
            # Round to nearest multiple of divisor
            rounded = round(dim / divisor) * divisor
            # Ensure minimum size
            if rounded < divisor:
                rounded = divisor
            compatible_size.append(rounded)
        return compatible_size

    compatible_patch_size = make_patch_size_compatible(patch_size)
    print(f"   Original patch size: {patch_size}")
    print(f"   Compatible patch size: {compatible_patch_size}")

    # Create fixed configuration that explicitly removes resampling
    fixed_config = {
        "inherits_from": "2d",
        "data_identifier": f"nnUNetPlans_{config_name}",
        "preprocessor_name": "DefaultPreprocessor",
        "batch_size": batch_size,
        "patch_size": compatible_patch_size,
        # Keep all other settings from original ResEnc config
        "median_image_size_in_voxels": current_config["median_image_size_in_voxels"],
        "spacing": current_config["spacing"],
        "normalization_schemes": current_config["normalization_schemes"],
        "use_mask_for_norm": current_config["use_mask_for_norm"],
        # Keep the improved ResidualEncoderUNet architecture
        "architecture": current_config["architecture"],
        "batch_dice": current_config["batch_dice"],
        # EXPLICITLY REMOVE RESAMPLING FUNCTIONS
        # DO NOT inherit resampling from base configuration
    }

    # Update the configuration
    plans["configurations"][config_name] = fixed_config

    # Create backup before modifying
    backup_path = f"{plans_dir}/nnUNetResEncUNetMPlans_no_resampling_backup.json"
    with open(backup_path, "w") as f:
        json.dump(plans, f, indent=2)
    print(f"‚úÖ Backup created: {backup_path}")

    # Save fixed ResEnc plans
    with open(resenc_plans_path, "w") as f:
        json.dump(plans, f, indent=2)

    print(f"‚úÖ ResEnc configuration fixed successfully!")
    print(f"   Configuration '{config_name}' updated in {resenc_plans_path}")
    print(f"   ‚úÖ Resampling functions REMOVED")
    print(f"   ‚úÖ Original image dimensions preserved")
    print(
        f"   ‚úÖ {compatible_patch_size[0]}√ó{compatible_patch_size[1]} patches will tile original images"
    )

    return True


def run_nnunet_preprocess(dataset_id: int, config_name: str):
    """
    Run nnUNetv2_preprocess command.

    Args:
        dataset_id: Dataset ID
        config_name: Configuration name
    """
    print(f"üîÑ Running nnUNetv2_preprocess...")
    print(f"   Dataset ID: {dataset_id}")
    print(f"   Configuration: {config_name}")
    print(f"   Plans: nnUNetResEncUNetMPlans")

    command = [
        "nnUNetv2_preprocess",
        "-d",
        str(dataset_id),
        "-c",
        config_name,
        "-p",
        "nnUNetResEncUNetMPlans",
    ]

    print(f"   Command: {' '.join(command)}")

    try:
        result = subprocess.run(command, check=True, capture_output=True, text=True)
        print(f"‚úÖ nnUNetv2_preprocess completed successfully!")
        print(f"   Output: {result.stdout}")
        return True
    except subprocess.CalledProcessError as e:
        print(f"‚ùå nnUNetv2_preprocess failed!")
        print(f"   Error: {e.stderr}")
        return False


def create_stratified_split(dataset_id: int, seed: int, tumor_percentage: float):
    """
    Create a stratified train/validation split with class balance control.

    Args:
        dataset_id: Dataset ID
        seed: Random seed for reproducibility
        tumor_percentage: Percentage of tumor cases in training set (0.0-1.0)
    """
    random.seed(seed)
    np.random.seed(seed)

    print(f"üîß Creating stratified split for Dataset{dataset_id:03d}")
    print(f"   Target tumor percentage in training: {tumor_percentage * 100:.1f}%")

    # Set up paths
    nnUNet_preprocessed = os.environ.get("nnUNet_preprocessed")
    if not nnUNet_preprocessed:
        # Set environment variables if not already set
        script_dir = Path(__file__).parent
        project_dir = script_dir.parent
        nnUNet_preprocessed = str(
            project_dir / "tumor_segmentation" / "data_nnUNet" / "preprocessed"
        )
        os.environ["nnUNet_preprocessed"] = nnUNet_preprocessed
        print(f"‚ö†Ô∏è  Set nnUNet_preprocessed to: {nnUNet_preprocessed}")

    # Dataset paths
    dataset_name = f"Dataset{dataset_id:03d}_TumorSegmentation"
    preprocessed_path = Path(str(nnUNet_preprocessed)) / dataset_name

    if not preprocessed_path.exists():
        print(f"‚ö†Ô∏è  Preprocessed directory not found: {preprocessed_path}")
        print("   This is expected if preprocessing hasn't been run yet.")

    # Get dataset path for raw data
    nnUNet_raw = os.environ.get("nnUNet_raw")
    if not nnUNet_raw:
        # Set environment variables if not already set
        script_dir = Path(__file__).parent
        project_dir = script_dir.parent
        nnUNet_raw = str(project_dir / "tumor_segmentation" / "data_nnUNet")
        os.environ["nnUNet_raw"] = nnUNet_raw
        print(f"‚ö†Ô∏è  Set nnUNet_raw to: {nnUNet_raw}")

    dataset_path = Path(str(nnUNet_raw)) / dataset_name

    # Get all training cases from the imagesTr directory
    images_tr_dir = dataset_path / "imagesTr"
    if not images_tr_dir.exists():
        raise FileNotFoundError(f"ImagesTr directory not found: {images_tr_dir}")

    # Get all image files and extract case names
    image_files = list(images_tr_dir.glob("*_0000.png"))
    all_cases = []

    for img_file in image_files:
        # Extract case name: patient_001_0000.png -> patient_001
        case_name = img_file.stem.replace("_0000", "")
        all_cases.append(case_name)

    # Sort for reproducibility
    all_cases.sort()

    print(f"üìä Dataset info:")
    print(f"   Total cases: {len(all_cases)}")

    # Separate patients and controls
    patients = [case for case in all_cases if case.startswith("patient_")]
    controls = [case for case in all_cases if case.startswith("control_")]

    print(f"   Patients: {len(patients)}")
    print(f"   Controls: {len(controls)}")

    # Check which patients actually have tumors
    labels_dir = dataset_path / "labelsTr"
    patients_with_tumors = []
    patients_without_tumors = []

    for patient in patients:
        label_file = labels_dir / f"{patient}.png"
        if label_file.exists():
            label = cv2.imread(str(label_file), cv2.IMREAD_GRAYSCALE)
            if label is not None:
                tumor_pixels = np.sum(label.astype(int) > 0)
                if tumor_pixels > 0:
                    patients_with_tumors.append(patient)
                else:
                    patients_without_tumors.append(patient)

    print(f"   Patients with tumors: {len(patients_with_tumors)}")
    print(f"   Patients without tumors: {len(patients_without_tumors)}")

    if len(patients_with_tumors) == 0:
        raise ValueError(
            "No patients with tumors found! Cannot create meaningful split."
        )

    # Create validation set: 20% of all patients (with and without tumors)
    all_patients = patients_with_tumors + patients_without_tumors
    random.shuffle(all_patients)
    val_split_point = int(0.2 * len(all_patients))
    val_patients = all_patients[:val_split_point]
    train_patients_available = all_patients[val_split_point:]

    # Separate available training patients by tumor status
    train_patients_with_tumors = [
        p for p in train_patients_available if p in patients_with_tumors
    ]
    train_patients_without_tumors = [
        p for p in train_patients_available if p in patients_without_tumors
    ]

    print(f"\nüìä Patient Split:")
    print(f"   Validation patients: {len(val_patients)} (20% of all patients)")
    print(
        f"   Available training patients with tumors: {len(train_patients_with_tumors)}"
    )
    print(
        f"   Available training patients without tumors: {len(train_patients_without_tumors)}"
    )

    # Calculate how many control cases we need to achieve the target tumor percentage
    tumor_cases_in_training = len(train_patients_with_tumors)
    if tumor_percentage > 0:
        target_control_cases = int(
            tumor_cases_in_training * (1 - tumor_percentage) / tumor_percentage
        )
    else:
        target_control_cases = 0

    print(f"\nüéØ Class Balance Calculation:")
    print(f"   Tumor cases available: {tumor_cases_in_training}")
    print(f"   Target control cases: {target_control_cases}")
    print(f"   Total control cases available: {len(controls)}")

    # Select control cases for training
    random.shuffle(controls)
    if target_control_cases > len(controls):
        print(
            f"‚ö†Ô∏è  Warning: Not enough control cases. Using all {len(controls)} available."
        )
        train_controls = controls
    else:
        train_controls = controls[:target_control_cases]

    # Combine training cases
    train_cases = (
        train_patients_with_tumors + train_patients_without_tumors + train_controls
    )
    random.shuffle(train_cases)

    # Calculate actual tumor percentage
    actual_tumor_cases = len(train_patients_with_tumors)
    actual_total_cases = len(train_cases)
    actual_tumor_percentage = (
        actual_tumor_cases / actual_total_cases if actual_total_cases > 0 else 0
    )

    print(f"\nüìä Final Split:")
    print(f"   Training cases: {len(train_cases)}")
    print(f"     - Patients with tumors: {len(train_patients_with_tumors)}")
    print(f"     - Patients without tumors: {len(train_patients_without_tumors)}")
    print(f"     - Controls: {len(train_controls)}")
    print(
        f"     - Actual tumor percentage: {actual_tumor_percentage * 100:.1f}% (target: {tumor_percentage * 100:.1f}%)"
    )

    print(f"   Validation cases: {len(val_patients)}")
    print(f"     - All patients (20% of total patients)")

    # Create the split structure
    stratified_split = [{"train": train_cases, "val": val_patients}]

    # Save the split file
    splits_file = preprocessed_path / "splits_final.json"
    save_json(stratified_split, str(splits_file))

    print(f"\n‚úÖ Stratified split file created: {splits_file}")
    print(
        f"   Split type: Stratified split with {tumor_percentage * 100:.1f}% tumor target"
    )
    print(f"   Validation: Patients only (20% of all patients)")

    return splits_file


def main():
    """Main function that runs the complete pipeline."""
    print("üöÄ Automated nnUNet Pipeline")
    print("=" * 50)
    print(f"Model Name: {MODEL_NAME}")
    print(f"Tiling Size: {TILING_SIZE[0]}√ó{TILING_SIZE[1]} pixels")
    print(f"Stratified Split: {STRATIFIED_SPLIT_PERCENTAGE * 100:.1f}%")
    print(f"Dataset ID: {DATASET_ID}")
    print(f"Batch Size: {BATCH_SIZE}")
    print(f"Random Seed: {RANDOM_SEED}")
    print()
    
    # Print detailed model configuration information
    print_model_info()
    print()

    # Step 1: Create fresh ResEnc model
    print("üìã Step 1: Creating fresh ResEnc model...")
    if not create_fresh_resenc_plans(
        DATASET_ID, TILING_SIZE, BATCH_SIZE, MODEL_NAME, SMALLER_MODEL_CONFIG
    ):
        print("‚ùå Failed to create fresh ResEnc model. Exiting.")
        return False
    print()

    # Step 2: Fix ResEnc configuration to remove resampling
    print("üìã Step 2: Fixing ResEnc configuration (no resampling)...")
    if not fix_resenc_no_resampling(DATASET_ID, TILING_SIZE, BATCH_SIZE, MODEL_NAME):
        print("‚ùå Failed to fix ResEnc configuration. Exiting.")
        return False
    print()

    # Step 3: Run nnUNetv2_preprocess
    print("üìã Step 3: Running nnUNetv2_preprocess...")
    if not run_nnunet_preprocess(DATASET_ID, MODEL_NAME):
        print("‚ùå Failed to run nnUNetv2_preprocess. Exiting.")
        return False
    print()

    # Step 4: Create validation split strategy
    print("üìã Step 4: Creating validation split strategy...")
    print("üõ°Ô∏è  OVERFITTING PREVENTION: Using enhanced validation strategy")
    print("üéØ  IMPORTANT: Using tumor-only validation to match test distribution")
    
    # Option 1: Cross-validation (recommended for overfitting prevention)
    print("   Tumor-only validation prevents distribution mismatch (test = 100% tumor)")
    print("   Training on all data (tumor + control) but validating only on tumor cases")
    print("   ‚Üí Using 5-fold Cross-Validation with tumor-only validation sets...")
    
    try:
        # IMPORTANT: Enable tumor-only validation since test set contains only tumor cases
        # This prevents distribution mismatch between training validation and test
        splits_file = create_cross_validation_splits(DATASET_ID, RANDOM_SEED, n_folds=5, tumor_only_validation=True)
        print(f"‚úÖ Cross-validation splits created: {splits_file}")
        print(f"   üí° This 5-fold CV approach helps prevent overfitting vs single split")
    except Exception as e:
        print(f"‚ö†Ô∏è  Cross-validation failed, falling back to stratified split: {e}")
        try:
            splits_file = create_stratified_split(
                DATASET_ID, RANDOM_SEED, STRATIFIED_SPLIT_PERCENTAGE
            )
            print(f"‚úÖ Stratified split created: {splits_file}")
            print(f"   ‚ö†Ô∏è  Single split used - monitor for overfitting!")
        except Exception as e2:
            print(f"‚ùå Failed to create any validation split: {e2}")
            return False
    print()

    # Success message
    print("üéâ Pipeline completed successfully!")
    print("=" * 50)
    print(f"‚úÖ Fresh ResEnc model '{MODEL_NAME}' created")
    
    # Print model configuration summary
    if SMALLER_MODEL_CONFIG:
        estimated_params = calculate_model_parameters(SMALLER_MODEL_CONFIG)
        standard_params = calculate_model_parameters(None)
        reduction = (1 - estimated_params / standard_params) * 100
        print(f"‚úÖ Model: {SELECTED_MODEL_SIZE.upper()} ({estimated_params/1_000_000:.1f}M params, {reduction:.1f}% reduction)")
    else:
        estimated_params = calculate_model_parameters(None)
        print(f"‚úÖ Model: STANDARD ({estimated_params/1_000_000:.1f}M params)")
    
    print(f"‚úÖ Configuration fixed (no resampling, {TILING_SIZE[0]}√ó{TILING_SIZE[1]} tiling)")
    print(f"‚úÖ Preprocessing completed")
    print(f"‚úÖ Stratified split created ({STRATIFIED_SPLIT_PERCENTAGE * 100:.1f}% tumor target)")
    print()

    print("üìã Next steps - OVERFITTING PREVENTION ENABLED:")
    print("=" * 50)
    print("üõ°Ô∏è  IMPORTANT: Your pipeline now includes overfitting prevention strategies!")
    print()
    print(f"# Train the model with cross-validation (RECOMMENDED):")
    print(f"nnUNetv2_train {DATASET_ID} {MODEL_NAME} 0 -p nnUNetResEncUNetMPlans --npz")
    print(f"nnUNetv2_train {DATASET_ID} {MODEL_NAME} 1 -p nnUNetResEncUNetMPlans --npz")
    print(f"nnUNetv2_train {DATASET_ID} {MODEL_NAME} 2 -p nnUNetResEncUNetMPlans --npz")
    print(f"nnUNetv2_train {DATASET_ID} {MODEL_NAME} 3 -p nnUNetResEncUNetMPlans --npz")
    print(f"nnUNetv2_train {DATASET_ID} {MODEL_NAME} 4 -p nnUNetResEncUNetMPlans --npz")
    print("   üí° Train all 5 folds to get robust performance estimates")
    print()
    print(f"# Validate with cross-validation:")
    print(f"nnUNetv2_train {DATASET_ID} {MODEL_NAME} all -p nnUNetResEncUNetMPlans --val --npz")
    print()
    print(f"# Find best configuration:")
    print(f"nnUNetv2_find_best_configuration {DATASET_ID} -c {MODEL_NAME} -p nnUNetResEncUNetMPlans")
    print()
    print(f"# Run inference:")
    print(f"nnUNetv2_predict -i INPUT_FOLDER -o OUTPUT_FOLDER -d {DATASET_ID} -c {MODEL_NAME} -p nnUNetResEncUNetMPlans")
    print()
    print("üö® OVERFITTING MONITORING:")
    print("   - Watch for validation loss plateauing (early stopping enabled)")
    print("   - Compare training vs validation DICE scores regularly")
    print("   - Expected: Training DICE should NOT reach 0.99 with these settings")
    print("   - Target: Validation DICE should be within 5-10% of training DICE")
    print()
    print("üìä PERFORMANCE EXPECTATIONS:")
    print(f"   - Model size: REDUCED ({SELECTED_MODEL_SIZE.upper()}) - ~75% fewer parameters")
    print("   - Data augmentation: ENHANCED (based on nnUNet BraTS research)")
    print("   - Validation: 5-FOLD CROSS-VALIDATION (nnUNet standard)")
    print("   - Tumor ratio: REDUCED (20% instead of 50% - more realistic)")
    if QUICK_TEST_MODE:
        print("   - Early stopping: ULTRA-AGGRESSIVE (5 epoch patience, 50 max epochs)")
    else:
        print("   - Early stopping: AGGRESSIVE (10 epoch patience, 1000 max epochs)")
    print()
    print("‚ö†Ô∏è  IF OVERFITTING PERSISTS:")
    print("   1. Use 'minimal' or 'tiny' model size")
    print("   2. Reduce batch size to 12 or 16")
    print("   3. Add more diverse training data")
    print("   4. Consider ensemble methods for better generalization")

    return True


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
